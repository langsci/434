\documentclass[output=paper,colorlinks,citecolor=brown]{langscibook}
\ChapterDOI{10.5281/zenodo.12665933}
\author{Steven T. Piantadosi\orcid{}\affiliation{UC Berkeley \& Helen Wills Neuroscience Institute} }

\title{Modern language models refute Chomsky's approach to language}
\abstract{Modern machine learning has subverted and bypassed the theoretical framework of Chomsky’s generative approach to linguistics, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, and I highlight the links between these models and approaches to linguistics that are based on gradient computations and memorized constructions. I also describe why these models undermine strong claims for the innateness of language and respond to several critiques of large language models, including arguments that they can’t answer ``why'' questions and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress.}

\IfFileExists{../localcommands.tex}{
   \addbibresource{../localbibliography.bib}
   \input{../localpackages}
   \input{../localcommands}
   \input{../localhyphenation}
   \boolfalse{bookcompile}
   \togglepaper[23]%%chapternumber
}{}
\begin{document}
\maketitle
\label{chap-16_piantadosi}

\section{Introduction}\largerpage

After decades of privilege and prominence in linguistics, Noam Chomsky's approach to the science of language is experiencing a remarkable downfall. The  story is, in part, a cautionary tale about what happens when an academic field isolates itself from what should be complementary endeavours. Chomsky's approach and methods are often argued to be problematic (e.g. \citealt{harris1993linguistics,pullum1989formal,behme2012potpourri,postal2012two,behme2014galilean}), but it is yet to be widely recognized just how the underlying ideas have been undermined by recent computational advances.

The state of the art in virtually all computational language tasks makes use of deep neural networks \citep{lecun2015deep}, often neural networks called \textit{transformers} \citep{vaswani2017attention,brown2020language,bommasani2021opportunities}. These are models of text that are trained on huge datasets of internet-based text to predict upcoming linguistic material (often at a sub-word but supra-character encoding). Several advances have allowed these models to become wildly successful. First, we have become able to train them on massive data sets. This is in part due to advances in computing (e.g. computing derivatives in arbitrary models) and in part the availability of huge collections of text from the internet. A typical language model might be trained on hundreds of billions of tokens, estimated to cost millions of dollars in energy alone. Second, the model architecture flexibly deals with nonlocal dependencies and allows the prediction of a word to potentially depend on material that is far away. The key consequence is that leading models are able to generate not only grammatical sentences, but entire discourses, scripts, explanations, poems, etc. This pursuit has been led in recent years by AI companies \citep{ahmed2023growing}.

A nice example can be provided by the following, produced by ChatGPT when prompted with only the text in italics. All examples in this paper show the first response generated by ChatGPT to these queries by the author in early 2023. Good queries often focus on weird or unusual requests in order to ensure that the output is well outside of the training set.

\begin{myquote}
\textit{Write a short story explaining how an ant could sink an aircraft carrier.}\medskip\\
\noindent Once upon a time, in a far-off land, there lived a colony of ants. These ants were not like any ordinary ants, they were highly intelligent and had the ability to communicate with one another telepathically. They had been observing the humans and their ways for a long time, and they had become fascinated by the massive aircraft carriers that the humans built and sailed across the seas.

One day, the leader of the ant colony, a wise old ant named Alex, had an idea. He called all the ants together and said, ``My friends, we have the ability to think and communicate like the humans do. We can also work together as a team to achieve great things. I propose that we build a ship of our own, a ship that can sink the human's aircraft carrier.''.

The other ants were shocked and amazed by Alex's proposal, but they were also excited by the idea. They immediately set to work, gathering twigs and sticks from the forest floor, and using their own tiny bodies to weave them together.

Months passed, and the ants' ship grew bigger and bigger. It was not as big as an aircraft carrier, but it was large enough to hold many ants and was heavily armed with miniature torpedoes and cannons.

Finally, the day arrived when the ant ship was ready. The ants climbed aboard and set sail towards the nearest aircraft carrier. As they approached the carrier, the humans were shocked to see such a small ship coming towards them. They laughed and jeered, thinking that the ants' ship was no threat to them.

But the ants were not to be underestimated. They had been studying the human's ship, and they knew its weak spots. As they neared the carrier, they launched their miniature torpedoes, which hit the carrier directly in its vulnerable areas. The carrier began to sink, and the humans panicked.

In the end, the ant ship emerged victorious, having successfully sunk the massive aircraft carrier. The ants returned home to their colony as heroes, and their story became a legend that was passed down through the generations.

And the human learned to never underestimate the power of collective intelligence and unity.

It is important to note that the story above is a fictional tale and it is impossible for an ant to sink an aircraft carrier, as in reality the size, strength and weapon capability of an aircraft carrier would far exceed the capability of a group of ants.
\end{myquote}


\noindent Ignore whatever theoretical persuasions you have for a moment and just recognize how \textit{remarkable} that response is. A coherent and grammatical story unfolds from a simple prompt, even though the requested scenario strains plausibility and probability. The response tackles the core essence of the question to provide a compelling story about how an ant \textit{might} sink an aircraft carrier. The answer is outside of the training set. In fact, after ``Once upon a time, in a far-off land, there lived a colony of ants'', a Google search returns no matching strings on the entire internet.

Anyone who has dealt in simpler language models like $n$-grams or context-free grammars will be impressed that this model is capable of incorporating not just longer dependencies within sentences, but across sentences. It also creates complex structures, with e.g. multiple embedded clauses, prepositional phrases, conjunctions, etc; it uses pronouns, determiners, quantifiers, adjectives, etc. in realistic ways; this and similar models have few apparent problems with agreement or pronoun reference \citep{gulordava2018colorless,goldberg2019assessing}. Any one of these abilities has been the topic of countless dissertations in linguistics. Beyond that, such models succeed on tasks that formal linguistic theories haven't yet dreamed of -- like interpreting the semantics of the words in question and maintaining not just coherent sentences but entire coherent discourses. Such models exhibit over $90\%$ accuracy in Winograd-style questions \citep{kocijan2020review,kocijan2022defeat}, which require interpreting language in world contexts (e.g. ``The trophy doesn’t fit in the brown suitcase because it is too large. What is too large?''). What is clear is that these models \textit{use language in a way that is remarkably human} \citep{mahowald2023dissociating}. 


This kind of example hearkens back to \citet{turing1950computing}, who considered asking an artificial intelligence to write a sonnet on the topic of ``The Forth Bridge'',\footnote{ According to Wikipedia, a cantilevered rail bridge from 1890 in Scotland that was voted in 2016 to be Scotland's greatest human-made wonder, apparently beating out Ewan McGregor.} and suggested that success in solving such tasks would demonstrate many key aspects of human-like intelligence. Contemporary language models can do much more -- they can write raps, explain things, imagine conversations, create a mash-up script of Seinfeld and Star Trek, etc. They are aware of even sub-word properties like rhymes:
\begin{myquote}
\textit{Come up with a limerick about syntactic island constraints in linguistics.}\medskip\\
\noindent On syntactic islands, we must be precise,\\
For constraints on movement can be quite nice.\\
But when the constraints are breached,\\
Our grammars are impeached,\\
And our linguistic analyses pay the price.
\end{myquote}

\noindent And they are aware of style:\largerpage[2]
\begin{myquote}
\textit{Explain the fundamental theorem of arithmetic in the style of Donald Trump.}\medskip\\
Well, let me tell you folks, the fundamental theorem of arithmetic is a really great theorem, it's tremendous, and it's all about prime numbers. Believe me, I know a lot about prime numbers. They're very important, very powerful, very beautiful numbers.

Now, the fundamental theorem of arithmetic tells us that every whole number greater than 1 can be expressed as a unique product of prime numbers. That's right, every single number can be broken down into its prime factors, and there's only one way to do it. It's like a code, a secret code that only primes can unlock.

This is a really amazing theorem, folks, and it's incredibly useful. It's used in cryptography, number theory, and all sorts of other fields. It's a cornerstone of mathematics, and it's something that we should all be very proud of.

So, in conclusion, the fundamental theorem of arithmetic is a fantastic theorem, it's tremendous, and it's all about prime numbers. I love prime numbers, they're the best. And this theorem tells us that every number can be broken down into primes in a unique way. It's a great thing, believe me
\end{myquote}

\noindent Note that this specific example was not in the model's training set -- there is no possibility that Trump understands prime numbers. However information about the fundamental theorem of arithmetic was, as was the speech style of Donald Trump. ``Believe me, I know a lot about prime numbers'' is probably not something Trump has actually said, but certainly something he \textit{would} say. 

Examples like these show why dismissals from cognitive scientists like Gary Marcus\footnote{ \url{https://garymarcus.substack.com/p/nonsense-on-stilts}} that the models are just the same as ``autocomplete'' systems on your phone are far too shallow. The model is able to put together things in its training \textit{in new ways} that maintain a considerable amount of linguistic and conceptual coherence. That requires more than merely guessing things it has seen before -- it requires modeling the dynamics of language. If models only repeated what they had seen before, they would not be able to generate anything new, particularly complex sentence structures that are grammatical and coherent. It is somewhat difficult to convey how remarkable the models are currently. You just have to interact with them. They are imperfect, to be sure, but my qualitative experience interacting with them is like talking to a child, who happened to have memorized much of the internet. 

\section{How this magic happens}

These tools are not just impressive, they are philosophically important. The reason they are important is that they succeed by following a very particular approach: they are trained \textit{only on text prediction}.\footnote{The underlying neural network weights are typically optimized in order to predict text, but note that many applications of these models also use human feedback to fine-tune parameters and try to tamp down the horrible things text on the internet leads models to say.} This means that the models form probabilistic expectations about the next word in a text and they use the true next word as an error signal to update their latent parameters. This idea is one that dates back to at least \citet{elman1990finding}, who showed how training a neural network on text prediction could lead it to discover key pieces of the underlying linguistic system. 

Modern models are a resounding scientific victory for Elman's idea. But while modern models inherit his general setup, advances have added a few critical differences. Probably the most important is that modern models include an \textit{attentional mechanism} that allows the next word in sequence to be predicted from some previous far in the past. For example, in the ant story above,  when it says that ``The other ants were shocked and amazed by \textit{Alex}'s \ldots'', it retrieves the name ``Alex'' from dozens of words prior. This likely is the key property that distinguishes large language models from the most popular earlier models. An {n}-gram model, for example, would estimate and use a conditional probability that depends on just the preceding few words (e.g. $n=2,3,4,5$); context-free grammars make independence assumptions that keep lexical items from influencing those far away. Not only do large language models allow such long-distance influences, but they allow them to take a relatively unconstrained form and so are able to induce functions which, apparently, do a stellar job at in-context word prediction. 

A second key feature of these models is that they \textit{integrate semantics and syntax}. The internal representations of words in these models are stored in a vector space, and the locations of these words include not just some aspects of meaning, but properties that determine how words can occur in sequence (e.g. syntax). There is a fairly uniform interface for how context and word meaning predicts upcoming material -- syntax and semantics are not separated out into distinguished components in the model, nor into separate predictive mechanisms. Because of this, the network parameters these models find blend syntactic and semantic properties together, and both interact with each other and the attentional mechanism in nontrivial ways. This doesn't mean that the model is incapable of distinguishing syntax and semantics, or e.g. mirroring syntactic structures regardless of semantics (see examples below), but it does mean that the two can be mutually informative. A related aspect of the models is that they have a huge memory capacity of billions to trillions of parameters. This allows them to memorize idiosyncrasies of language, and in this way they inherit from a tradition by linguists who have emphasized the importance of constructions \citep{adele1995constructions,jackendoff2013constructions,goldberg2006constructions,goldberg2003constructions,tomasello2000item,mccauley2019language,tomasello2005constructing,edelman2007behavioral} (see \citealt{weissweiler2023construction} for construction grammar analyses of large language models). Such models also inherit from the tradition of learning bottom-up, from data (e.g. \citealt{bod2003data,solan2005unsupervised}), and computational work which explicitly connects syntax and semantics \citep{steedman2001syntactic,siskind1996computational,ge2005statistical,kwiatkowski2012probabilistic,liang2009learning}. The attentional mechanism seems to function in some ways like dependency grammar \citep{
tesniere1959elements,hays1964dependency,de2021universal}.

A good mental picture to have in mind for how massively over-parameterized models like these work is that they have a rich potential space for inferring \textit{hidden variables and relationships}. Hidden (or latent) variables have been one of the key aspects of language that computational and informal theories alike try to capture \citep{pereira2000formal,linzen2021syntactic}. In the middle of a sentence, there is a hidden variable for the latent structure of the sentence; in speaking an ambiguous word, we have in mind a hidden variable for which meaning we intend; throughout a discourse we have in mind a larger story arc that only unfolds across multiple sentences. The formalisms of linguistics attempt to characterize these hidden variables too. But what large language models do is \textit{infer} likely hidden structure because that structure permits them to better predict upcoming material. This makes them conceptually similar to embedding theorems in mathematics \citep{packard1980geometry,takens1981detecting,ye2016information}, which show that sometimes the full geometry of a dynamical system can be recovered from a low-dimensional projection of its states evolving in time. Linguistic corpora are a low-dimensional projection of both syntax and thought, so it is not implausible that a smart learning system could recover at least some aspects of these cognitive systems from watching text alone \citep{piantadosi2022meaning}.

\begin{sloppypar}
The structures present in large language models can be seen in detailed analyses, where as the model generates text after training, its internal states represent latent aspects of syntactic structure and semantic meaning \citep{manning2020emergent,futrell2019neural,linzen2021syntactic,pavlick2022semantic}. The structure of the model's internal representation states and attentional patterns after training comes to capture tree structures with strong similarities to human-annotated parse trees \citep{manning2020emergent}, and the degree to which a model is tree-structured even predicts its generalization performance \citep{murty2022characterizing}. The models seem to perform well on constructions involving tracking the right latent state, like function words \citep{kim2019probing} and filler-gap dependencies \citep{wilcox2018rnn}. In fact, the internal processing structure of some models seems to spontaneously develop an intuitive pipeline of representing parts of speech, followed by parsing, semantic analysis, etc. \citep{tenney2019bert,liu2019linguistic}.
\end{sloppypar}

 
All of this is possible because large language models develop representations \textit{of} key structures and dependencies. It's just that these representations are parameterized in a way which is unfamiliar to linguistics. As argued by \citet{baroni2022proper}, this means that language models should be treated as bona fide linguistic \textit{theories}. Specifically, a space of possible theories is parameterized by the models and compared to data to find which theory is best in a formal sense. To make one version of the idea concrete, imagine a physicist who wasn't sure whether the law of gravitation force fell off with distance $1/r$ or distance squared $1/r^2$. To decide, the physicist might formulate a super-equation that captured both possibilities, for instance,
\[
F(r,\alpha) = \alpha \cdot \frac{1}{r} + (1-\alpha) \cdot \frac{1}{r^2}.
\]
By seeing which parameter $\alpha$ best fits empirical data (e.g. measurements of forces $F$ and distances $r$), they are \textit{comparing} these two theories: $\alpha\approx 1$ means the former theory is right (since then $F=1\cdot (1/r)+0$) and $\alpha \approx 0$ means the latter (since $F=0+1\cdot(1/r^2)$). When the data are stochastic, a good way to measure how well any particular $\alpha$ does is to see what probability it assigns to the data. We can make a principled choice between parameters -- and thus theories -- by choosing the one that makes the data most likely (\textit{maximum likelihood principle}), although often including some prior information about plausible parameter values or penalties on complexity (e.g. Bayesian estimation). 
Such a physicist might then find that the best parameter for capturing data has $\alpha \approx 0$, supporting the second theory.\footnote{For decades, other fields have used statistical learning models that take empirical data and infer laws \citep{koza1994genetic,langley1983rediscovering,schmidt2009distilling,udrescu2020ai}.} In this case, inferring parameters \textit{is} comparing theories: in computational modeling, there is no bright line between ``just'' fitting parameters and advancing theory.\largerpage

Something very similar happens with many machine learning models; the main difference is that in these models, we don't explicitly or intentionally ``build in'' the theories under comparison ($1/r$ and $1/r^2$). There exist natural bases from which you can parameterize essentially \textit{any} computational theory.\footnote{Up to the capacity of the network.} Parameter fitting in these models is effectively searching over a huge space of possible theories to see which one works best, in a well-defined, quantitative sense. 

The bases required are actually pretty simple. Polynomials are one, but neural networks with sigmoid activations are another: fitting parameters in either of these can, in principle, realize countless possible relationships in the underlying domain. The challenge is that when these universal bases are used, it requires extra scientific work to see and understand what the parameters mean. Just to illustrate something roughly analogous, if we happened to write the above equation in a less transparent way, 
\[
F(r,\alpha) = \frac{\alpha \cdot (r-1) + 1}{\log \left(\left(e^r\right)^{r}\right)},
\]
then it might take some work to figure out which $\alpha$ values correspond to $1/r$ and which to $1/r^2$. Squint just a little and you can imagine that instead of algebra, we had a mess of billions of weighted connections between sigmoids to untangle and interpret. It becomes clear that it could be hard to determine what is going on, even though \textit{the theory is certainly in there}.

In fact, we don't deeply understand \textit{how} the representations these models create work (see \citealt{rogers2021primer}). It is a nontrivial scientific program to discover how their internal states relate to each other and to successful prediction. Researchers have developed tools to ``probe'' internal states (e.g. \citealt{belinkov2019analysis,tenney2019you,kim2019probing,linzen2021syntactic,warstadt2022artificial,pavlick2022semantic}) and determined some of the causal properties of these models. At the same time, this does not mean we are ignorant of all of the \textit{principles} by which they operate. We can tell from the engineering outcomes that certain structures work better than others: the right attentional mechanism is important \citep{vaswani2017attention}, prediction is important, semantic representations are important, etc. The status of this field is somewhat akin to the history of medicine, where people often worked out what kinds of treatments worked well (e.g. lemons treat scurvy) without yet understanding the mechanism. 

One thing that is interesting is how modern language models integrate varied computational approaches to language, not by directly encoding them, but by allowing them to emerge \citep{manning2020emergent,mcclelland2010emergence} from the architectural principles that \textit{are} built-in \citep{elman1996rethinking}. For example, the models appear to have representations of hierarchy \citep{manning2020emergent} and recursion, in the sense that they know about e.g. embedded sentences and relative clauses. They also almost certainly have analogs of constraints, popular in approaches like harmonic grammar \citep{smolensky2006harmonic,prince1997optimality}, model-theoretic grammar \citep{pullum2007evolution,pullum2013central,muller2023grammatical}, and HPSG \citep{muller2021head,dalrymple2022handbook}. 
The models likely include both hard constraints (like word order) and violable, probabilistic ones \citep{rumelhart1986learning}. They certainly memorize some constructions \citep{adele1995constructions,sag2012sign,jackendoff2013constructions,goldberg2006constructions,goldberg2003constructions,tomasello2000item,edelman2007behavioral} (see \citealt{chang2023speak}). All of those become realized in the parameters in order to achieve the overarching goal of predicting text well.


\section{The status of large language models as scientific theories}\largerpage

Many in language science see such models as at least relevant in some way to the future \citep{bommasani2021opportunities,baroni2022proper,pater2019generative}. After all, they are the only  models in existence that do a good job of capturing the basic dynamics of human language. However, in virtue of being neural networks, their~-- at least initial~-- state is wholly unlike the rules and principles that have dominated generative approaches to language.\footnote{I will use ``generative'' to refer to the mainstream, Chomskyan approaches like Government and Binding and Minimalism. Note that some other approaches are also considered ``generative'', and they may work from different, often more plausible, starting assumptions.} As described above, their parameters come to embody a theory of language, including representations of latent state through a sentence and a discourse. The exact same logic of tuning parameters to formalize and then compare theories is found in other sciences, like modeling hurricanes or pandemics: any set of assumptions will generate a distribution of predictions and the assumptions are adjusted to make the best predictions possible. In this way, a learning mechanism configures the model itself in the space of theories in order to satisfy the desired objective function. For hurricanes or pandemics, this is as rigorous as science gets; for sequences of words, everyone seems to lose their mind.

In discussing GPT-3 with Gary Marcus,\footnote{ \url{https://garymarcus.substack.com/p/noam-chomsky-and-gpt-3}} for example, the most positive thing Chomsky could say was that it has an ``ability to mimic some regularities in data'', followed quickly by ``In fact, its only achievement is to use up a lot of California’s energy''.\footnote{ One of the many limitations, concerns, and dangers \citep{bender2021dangers,bommasani2021opportunities} for these models is that they consume a lot of energy \citep{strubell2019energy}. It's estimated that these models take around 1000 MWh, compared to CA's daily generation of about 750,000 MWh -- so one model takes about $1/750$th of one day of CA's power.} In another interview, he summarized that the models ``have achieved zero'' in terms of our understanding of language. \citet{chomsky2023false} characterized the models as useful ``in some narrow domains'' but hampered by ``ineradicable defects'' that made them ``differ profoundly from how humans reason and use language''. As was quickly pointed out online, several of the examples they brought up -- like reasoning with counterfactuals or understanding sentences ``John is too stubborn to talk to'' -- current models actually get correct. \citet{chomsky2023false} and others critique an imagined version of these models, while ignoring the fact that the real ones so aptly capture syntax, a success many have persistently claimed was impossible.\largerpage

Part of why some generative linguists dismiss these models is that they are seen as too unconstrained, and thus not explanatory. Writing to Marcus about the models, Chomsky explains, 
\begin{quote}
You can’t go to a physics conference and say: I’ve got a great theory.  It accounts for everything and is so simple it can be captured in two words: ``Anything goes.''
% This quote is the original. Dot inside. St. Mü. 03.09.2024
% Source should be given.
% https://garymarcus.substack.com/p/noam-chomsky-and-gpt-3    

All known and unknown laws of nature are accommodated, no failures.  Of course, everything impossible is accommodated also.
\end{quote}
This critique is a familiar rephrasing of his (and others') comments on language learning -- essentially that one should not study an unconstrained system because it will not explain why languages have the particular form that they do.\footnote{ You have to wonder how a physics conference would react to someone saying, following  \citet{lasnik2002minimalist}, ``I've got a great theory. It accounts for everything: physical laws `might be a computationally perfect solution to the problem' of how objects move.''.}

But it is too coarse a gloss to dismiss modern language models as ``anything goes''. First, there is some ambiguity, in that as a model of language, the \textit{trained} model is no longer ``anything goes'' -- they implement one specific computation. But on the learning side, it's important to realize that there is no such thing as an ``anything goes'' model, in that any model will necessarily have certain tendencies and biases. Another way to say this is that not all ``anything goes'' models are equivalent. A three-layer neural network is well-known to be capable of approximating any computable function \citep{siegelmann1995computational}. That's also an ``anything goes'' model. But the three-layer network will \textit{not} work well on this kind of text prediction. Indeed, even some earlier neural network models, LSTMs, did not do as well \citep{futrell2019neural,marvin2018targeted,hu2020systematic};  architectures generally vary in how well they capture  computational classes of string patterns (e.g. \citealt{deletang2022neural}).\footnote{ Some also consider them not to be ``scientific'' theories because they are \textit{engineered}. In an interview with Lex Fridman, Chomsky remarked, ``Is [deep learning] engineering, or is it science? Engineering, in the sense of just trying to build something that's useful, or science, in the sense that it's trying to understand something about elements of the world  \ldots{}  We can ask that question, is it useful? Yeah, it's pretty useful. I use Google Translator. So, on engineering grounds it's kinda worth having, like a bulldozer. Does it tell you anything about human language? Zero, nothing.'' In practice, there is often no clear line between engineering and science because scientists often need to invent new tools to even formulate theories: was Newton's calculus engineering instead or science? The machinery of transformational grammar? While the recent successes are due to engineering advances, researchers have been arguing for this form of model \textit{as} cognitive theories for decades.}

We are granted scientific leverage by the fact that models that are equally powerful in principle perform differentially. In particular, we may view each model or set of modeling assumptions as a possible hypothesis about how the mind may work. Testing how well a model matches human-like behavior then provides a scientific test of that model's assumptions. This is how, for example, the field has discovered that attentional mechanisms are important for performing well.  Similarly, ``ablation'' experiments allow researchers to alter one part of a network and use differing performance to pinpoint what principles support a specific behavior (see \citealt{warstadt2022artificial}). 

Even when -- like all scientific theories -- we discover how they fail to match people in terms of mechanism or representation, they still are informative. Heeding George Box's advice that ``all models are wrong, some are useful'', we can think about the scientific strengths, contributions, and weaknesses of these models without needing to accept or dismiss them entirely. In fact, these models have already made a substantial scientific contribution by helping to delineate what is \textit{possible}: Could it be possible to discover hierarchy without it being built in? Could word prediction provide enough of a learning signal to acquire most of grammar? Could a computational architecture achieve competence on WH-questions without movement, or use pronouns without innate binding principles? The answer to all of these questions is shown by recent language models to be ``yes''. 

Beyond that, the models embody several core desiderata of good scientific theories. First, they are
\textit{precise and formal} enough accounts to be implemented in actual computational systems, unlike most parts of generative linguistics \citep{pullum1989formal}. Implementation permits us to see that these theories are internally consistent and logically coherent. In virtue of being implemented, such models are able to \textit{make predictions}. Just to list a few examples, the patterns of connectivity and activation within large language models appear to capture dependency structures in words via attention \citep{manning2020emergent}. Their predictability measures can be compared to psychological measures \citep{hoover2022plausibility,shain2022large}. Transformer models ``predict nearly 100\% of explainable variance in neural responses to sentences'' \citep{schrimpf2021neural}. 

Unlike generative linguistics, these models show promise in being \textit{integrated with what we know about other fields}, specifically cognition and neuroscience. Many authors interested in human  concepts have investigated the vector representations that the models form \citep{lake2021word,bhatia2022transformer}. Surprisingly or not, the language model vectors appear to encode at least some aspects of semantics \citep{maas2011learning,socher2013recursive,bowman2015recursive,grand2022semantic,bhatia2022transformer,bhatia2022transformer,piantadosi2022meaning,dasgupta2022language,petersen2022lexical,pavlick2022semantic}, building on earlier models that encoded semantics in neural networks (e.g. \citealt{rogers2004semantic,elman2004alternative,mikolov2013distributed}). In fact, their semantic spaces can be aligned with the world with just a few labeled data points, at least in simple domains \citep{patel2022mapping}. The representations that they learn can also be transferred to some degree across languages \citep{pires2019multilingual,chi2020finding,gonen2020s,papadimitriou2020learning,papadimitriou2021deep,hill2017representational}, suggesting that they are inferring something deep about meaning. Following leading theories of concepts \citep{block1986advertisement,block1998conceptual}, the representations that language models learn may be meaningful in the sense of maintaining nontrivial \textit{conceptual roles} \citep{piantadosi2022meaning}, contrary to claims that meaning requires connections to the real world \citep{bender2020climbing}. Building on the ``parallel and distributed'' tradition of cognitive modeling \citep{mcclelland1986parallel}, modern deep learning models are also likely able to be integrated with neuroscientific theories \citep{marblestone2016toward,richards2019deep,kanwisher2023using,mcclelland2020placing}. In particular, they make predictions about neural data (e.g. \citealt{schrimpf2021neural,caucheteux2022deep,goldstein2022shared}). Generative theories of syntax, by contrast, suffer from a ``chronic lack of independent empirical support'' and in particular have not been compellingly connected to neuroscience \citep{edelman2019verbal}.\footnote{``Considering how central the existence of a brain basis for syntax is to Chomskian (bio)linguistics, the scarcity of behavioral and brain evidence for syntactic structures is striking.  \ldots{}  In comparison to the basic phrase structure, evidence supporting the reality of more far-fetched theoretical constructs, such as movement, traces/copies, etc., remains elusive'' \citep{edelman2019verbal}.}

\begin{sloppypar}
Moreover, these models \textit{are empirically tested}, especially as a theory of grammar. Modern language models are state of the art in most natural language processing domains \citep{bommasani2021opportunities}. Approaches from generative approaches to syntax are not competitive in any domain and arguably have avoided empirical tests of their core assumptions \citep{edelman2003seriously}. Several authors have sought to quantitatively evaluate large language models on the syntactic constructions that motivate much of linguistic theory. Early results found some successes across a variety of different architectures. For example, \citet{warstadt2019neural} evaluated LSTM neural network models on a corpus of more than ten thousand acceptability judgments published in the linguistics literature. They found that these models perform about in the mid $70\%$ range, compared to human reliabilities in the upper $80\%$ to $90\%$ range. \citet{warstadt2019investigating} look at BERT embeddings on negative polarity items and find ``significant knowledge'' of these structures but that success depends heavily on how the structures are tested. More recently, GPT models show high performance on filler-gap structures, including various forms of islands \citep{wilcox2018rnn,wilcox2022using}.
\end{sloppypar}

\citegen{gauthier2020syntaxgym} \textit{SyntaxGym} is a standardized environment for testing models against a number of standardized test suites that captures linguistic constructions and phenomena like clefts, center-embedding, cataphors, negative polarity items, filler-gap dependencies, subordination, agreement, etc. This project builds on similarly exciting benchmarking efforts in neuroscience \citep{schrimpf2020integrative}. As of the writing of this article, the state of the art for language was a variant of GPT-2 that achieved nearly $90\%$ on these constructions. SyntaxGym is an ingenious resource that finally allows quantitative comparison of theories. A general trend is that the more recent language models perform better, though it appears that proponents of (e.g.) minimalist grammars have not compared theories on SyntaxGym. It is not clear there has ever been, in the history of science, an implemented computational system that achieves such accuracy, but which is dismissed by a field which has failed to develop even remotely comparable alternatives.

In the spirit of considering large language models as scientific theories, it's worth also highlighting their limitations. One is that while they succeed at language modeling, they are currently less successful in domains that require reasoning or thinking \citep{mahowald2023dissociating,lake2021word,barrett2018measuring,collins2022structured}. From an acquisition perspective, likely the most important limitation of current models is that they are trained on truly titanic datasets compared to children, by a factor of at least a few thousand (see \citealt{warstadt2022artificial} for a comprehensive review of models in language acquisition). Moreover, these datasets are strings on the internet rather than child-directed speech. Work examining the scaling relationship between performance and data size shows that at least current versions of the models do achieve their spectacular performance only with very large network sizes and large amounts of data. \citep{kaplan2020scaling} However, \citet{zhang2020you} show that actually most of this learning is not about syntax. Models that are trained on 10--100 million words ``reliably encode most syntactic and semantic features'' of language, and the remainder of training seems to target other skills (like knowledge of the world). This in fact matches in spirit analyses showing that syntactic knowledge requires a small number of bits of information, especially when compared to semantics \citep{mollica2019humans}. \citet{hosseini2022artificial} present evidence that models trained on developmentally-plausible amounts of data already capture human neural responses to language in the brain. 

Importantly, as \citet{warstadt2022artificial} outline, these models are in their early stages of development, so their successes are likely to be more informative about the path of children's language acquisition than the models' inevitable limitations. Current models provide a lower bound on what is possible, but even the known state-of-the-art doesn't characterize how well future models may do. Our methods for training on very small datasets will inevitably improve. One improvement might be to build in certain other kinds of architectural biases and principles; or it might be as simple as finding better optimization or regularization schemes. Or, we might need to consider learning models that have some of the cognitive limitations of human learners, as in \citegen{newport1990maturational} ``less is more'' hypothesis. Such questions inspire the current ``The BabyLM Challenge'' \citep{warstadt2023call}, which aims to develop models capable of learning with a developmentally-plausible amount of data (see \citealt{geiping2022cramming} for training models with small amounts of compute resources; see also \citealt{eldan2023tinystories}). It is an interesting scientific question whether low-resource, low-data learning is possible -- I'll preregister a prediction of yes, with small architectural tweaks.

\section{The refutation of key principles}

The success of large language models is a failure for generative theories because it goes against virtually all of the principles these theories have espoused.  In fact, \textit{none} of the principles and innate biases that those who work in that tradition have long claimed necessary needed to be built into these models (e.g. binding principles, binary branching, island constraints, empty category principle, etc.). Moreover, these models were created without incorporating any of Chomsky's key methodological claims, like ensuring the models properly consider competence vs. performance, respect ``minimality'' or ``perfection'', and avoid relying on the statistical patterns of unanalyzed data.

The next sections focus on a few examples. 

\subsection{Syntax is integrated with semantics}
Chomsky and others have long emphasized the study of syntax as a separate entity, not only from the rest of cognition but from the rest of language (see, e.g. \citealt{chomsky1957syntactic,croft1995autonomy,newmeyer1991functional,adger2018autonomy}). Syntax in this approach is not supposed to be reducible to general statistics between words\footnote{``There is no current theory of linguistics
that takes human syntactic capacities to be reducible to general probabilistic relations between elements of unanalysed data'' \citep{adger2018autonomy}.} -- exactly the thing that large language models now provide. 

Modern large language models integrate syntax and semantics in the underlying representations:
encoding words as vectors in a high-dimensional space, without an effort to separate out e.g. part
of speech categories from semantic representations, or even predict at any level of analysis other
than the literal word. Part of making these models work well was in determining how to encode
semantic properties into vectors, and in fact initializing word vectors via encodings of
distribution semantics from e.g. \citet{mikolov2013distributed} and \citet{radford2019language}. Thus, an assumption of the autonomy of syntax is not required to make models that predict syntactic material and may well hinder it. 

We can have fun with the \textit{colorless green ideas} sentence (see \citealt{gulordava2018colorless}) commonly offered as an example that syntax functions separately from semantics (and that transitional probability statistics are not enough). The claim that this is out of reach of statistical models (e.g. \citealt{chomsky1957syntactic}) has been shown to be wrong \citep{pereira2000formal,norvig2012colorless,gulordava2018colorless}, but it is an interesting example nonetheless, in part because ChatGPT can not only learn relevant statistics, but it can create -- in this case surely mostly regurgitate -- an explanation of why this sentence is interesting:

\begin{myquote}
\textit{Why is the sentence ``colorless green ideas sleep furiously'' interesting?}\medskip\\
\noindent The sentence ``colorless green ideas sleep furiously'' is similarly interesting because it is a grammatically correct sentence, but it is semantically meaningless. The words used in the sentence are arranged in a way that follows standard syntax and grammar rules, but they do not form a coherent or understandable concept. This makes it an interesting linguistic oddity and a popular example of a nonsensical sentence.
\end{myquote}

\noindent We can test generalization with a follow-up:

\begin{myquote}
\textit{Generate ten other sentences like this}\medskip\\
\noindent Purple fluffy clouds dream wildly\\
Blue glittery unicorns jump excitedly\\
Orange sparkly penguins waddle energetically\\
Red shimmering elephants trumpet melodiously\\
Yellow dazzling lions roar majestically\\
Green glittery monkeys swing energetically\\
Pink sparkly giraffes walk gracefully\\
Black shiny kangaroos hop playfully\\
White glittery bears cuddle affectionately\\
Brown shimmering rabbits hop merrily.
\end{myquote}

\noindent This model successfully gets the referent of ``this'' (a sentence). The model resolves the ambiguity in ``like this'' to refer to the \textit{structure} of the sentence -- the exact thing that statistical models weren't even supposed to know! It generates some but not entirely low-frequency bigrams in the sentence. We can note a weakness in that it does not as readily generate wholly meaningless sentences (``black shiny kangaroos'' is rare but not impossible), likely because meaningless language is rare in the training data. These results illustrate that even models which integrate syntax and semantics are able to make syntactic generalizations when appropriate. Syntax need not be autonomous in the underlying mechanism or model's analysis in order to show this behavior. 

\subsection{Probability and information are central}

A second point is that for these models, probability and information theory are central. Chomsky has long been dismissive of probability, saying ``it must be recognized that the notion of ‘probability of a sentence’ is an entirely useless one, under any known interpretation of this term'' \citep{chomsky1969quine}, a position he has maintained for decades \citep{norvig2012colorless}.\footnote{Or in \citew{chomsky1957syntactic}, ``I think that we are forced to conclude that grammar is autonomous and independent of meaning, and that probabilistic models give no particular insight into some of the basic problems of syntactic structure.''} Often when those who work in Chomsky's 
tradition talk about probability models, they refer to simple things like \citegen{shannon1948mathematical} $n$-gram models that count up sequential word co-occurrences and were long used in natural language processing tasks \citep{chen1999empirical,manning1999foundations}. But by now, such models are decades out of date. 

Newer models use probability to infer entire generating processes and structures, a common cognitive task and modeling domain (e.g. \citealt{tenenbaum2011grow,ullman2012theory,lake2015human,goodman2011learning,lake2017building,rule2020child,kemp2008discovery,yang2022one}); such models build on experimental work documenting statistical learning in human learners (e.g. \citealt{saffran1996statistical,saffran1996word,aslin1998computation,newport2004learning,aslin2012statistical}). Probability is central for models because a probabilistic prediction essentially provides an error signal that can be used to adjust parameters that themselves encode structure and  generating processes. An analogy is that one might imagine watching a driver and inferring the relevant structures and dynamics from observation -- rules of the road (which side you drive on), conventions (behavior of multiple cars at stop signs), hard and soft constraints (don't turn too hard), etc. Even a simple domain like this faces many of the problems of undetermination seen in language, but it is one where it is easy to imagine a skilled scientist or anthropologist discovering the key elements by analyzing a mass of data. Something similar goes on in machine learning, where a space of possible rules is implicitly encoded into the parameters of the model (see above). 

It is worth noting that most models which deal in probabilities actually work with the $\log$ of probabilities, for reasons of numerical stability. Models that work on log probabilities are actually working in terms of description length \citep{shannon1948mathematical,cover1999elements}: finding parameters which make the data most likely (maximizing probability) is the same as finding parameters which give the data a short description (minimizing description length or complexity). Thus, the best parameters are equivalent to scientific theories that do a good job of \textit{compressing} empirical data in the precise sense of description length. Far from ``entirely useless'', probability is the measure that permits one to actually quantify things like complexity and minimality. 

\subsection{Representations are continuous and gradient}

The fact that predictions are probabilistic is useful because it means that the underlying representations are continuous and gradient. Unlike work formalizing discrete rules and processes, typical of generative linguistics (e.g. \citealt{chomsky1956three,chomsky1995minimalist,collins2016formalization,chomsky1957syntactic,pinker1988language}), modern language models do not use (at least explicit) rules and principles -- they are based in a continuous calculus that allows multiple influences to have a gradient effect on upcoming linguistic items. The foundation for this approach was laid by early modelers like \citet{rumelhart1986learning}, who argued for the key features of today's architectures decades ago, including that ``cognitive processes are seen as graded, probabilistic, interactive, context-sensitive and domain-general'' \citep{mcclelland2002rules}.

Continuity is important because it permits the models to use gradient methods -- essentially a trick of calculus -- to compute what direction to change all the parameters in order to decrease error the fastest. Tools like TensorFlow and PyTorch that permit one to take derivatives of arbitrary models have been a critical methodological advance. This is not to say that these models end up with no discrete values -- after all, they robustly generate subjects before verbs when trained on English. Similarly, the $F(r,\alpha)$ example might end up with a discrete answer like $\alpha \approx 0$. The key is that discreteness is a special case of continuous modeling, meaning that theories which work with continuous representations get the best of both worlds, fitting discrete patterns when appropriate and gradient ones otherwise. The success of gradient models over deterministic rules suggests that quite a lot of language is based on gradient computation. The success actually mirrors the prevalence of ``relaxation'' methods in numerical computing, where an optimization problem with hard constraints is often best solved via a nearby soft, continuous optimization problem. Thus, contrary to the intuition of many linguists, \textit{even if} we wanted a hard, discrete grammar out at the end, the best way for a learner to get there might be via a continuous representation.

\subsection{Learning succeeds in an unconstrained space}

Perhaps most notably, modern language models succeed despite the fact that their underlying architecture for learning is relatively unconstrained. This is a clear victory for statistical learning theories of language (see \citealt{contreras2023large}). These models are capable of fitting a huge number of possible patterns, and while the principles of their architecture do constrain them to make some patterns easier than others, the resulting systems are incredibly flexible. Despite this lack of constraint, the model is able to figure out much of how language works. One should not lose sight of the role that ``poverty of the stimulus'' arguments have long played for generative linguists (e.g. \citealt{lasnik2016argument,crain2001nature,legate2002empirical,wexler1980formal,laurence2001poverty,pearl2022poverty,crain2002language}). Poverty of the stimulus claims have been compellingly challenged both on empirical grounds about the nature of input, and through learning theories that acquire the relevant structures from input (e.g. \citealt{pullum2002empirical,clark2010linguistic,perfors2011learnability,reali2005uncovering,solan2005unsupervised}). Large language models essentially lay this issue to rest because they come with none of the constraints that others have insisted are necessary, yet they capture almost all key phenomena (e.g. \citealt{wilcox2022using}). It will be important to see, however, how well they can do on human-sized datasets, but their ability to generalize to sentences outside of their training set is auspicious for empiricism.

Recall that many of the learnability arguments were \textit{supposed} to be mathematical and precise, going back to \citet{gold1967language} (though see \citealt{johnson2004gold,chater2007ideal}) and exemplified by work like \citew{wexler1980formal}. It's not that we don't know the right learning mechanism; it's supposed to be that it can be proven none exists. Even my own textbook from undergraduate syntax purports to show a ``proof'' that because infinite, productive systems cannot be learned, parts of syntax must be innate \citep{carnie2021syntax}. \citet{legate2002empirical} call the innateness of language ``not really a hypothesis'' but ``an empirical conclusion'' based on the strength of poverty of stimulus arguments. Proof of the impossibility of learning in an unrestricted space was supposed to be the power of this approach. It turned out to be wrong.

The notion that the core structures of language could be discovered without substantial constraints may sound impossible to anyone familiar with the generative syntax rhetoric. But learning without constraints is not only possible, it has been well-understood and even \textit{predicted}. Formal analyses of learning and inference show that learners can infer the correct theory out of the space of possible computations \citep{solomonoff1964formal,hutter2004universal,legg2007universal}. In language specifically, the correct generating system for grammars can similarly be discovered out of the space of all computations (the most unrestricted space possible), using only observations of positive evidence \citep{chater2007ideal}.

In this view, large language models function somewhat like automated scientists or automated linguists, who also work over relatively unrestricted spaces, searching to find theories which do the best job of parsimoniously predicting observed data. It's worth thinking about the standard lines of questioning generative syntax has pursued -- things like, why don't kids ever say ``The dog is believed's owners to be hungry'' or ``The dog is believed is hungry'' (see \citealt{lasnik2016argument}). The answer provided by large language models is that these are not permitted under the best theory the model finds to explain what it does see. Innate constraints are not needed.

\subsection{Representations are complex, not minimal}

Next, there is an important sense in which large language models are not minimal representationally, but maximal. What I mean is that there is not a single core nugget of representation or structure (like \textit{merge}) that leads these models to succeed. Nor are any biases against derivational complexity likely to play a key role, since everything is a single big matrix calculation. This calculation moreover is not structurally minimal or ``perfect'' in the sense that minimalist linguistics means (e.g. \citealt{lasnik2002minimalist}). Instead, the attentional mechanisms of large language models condition on material that is arbitrarily far away, and perhaps not even structurally related since this is how they model discourses between sentences. A grammatical theory that matches people's almost limitless capacity for memorizing countless chunks of language changes the landscape of how we should think about derivation and complexity. 

Deep learning has actually changed how people think about complexity in statistical learning too. It has long been observed that having too many parameters in a model would prevent the model from generalizing well: too many parameters allow a model to fit patterns in the noise, and this can lead it to extrapolate poorly. Deep learning turned this idea on its head by showing that some models will fit (memorize) random data sets \citep{zhang2021understanding}, meaning they can fit all the patterns in the data (including noise) \textit{and} still generalize well. The relationship between memorization and generalization is still not well-understood, but one of the core implications is that statistical learning models can work well, sometimes, even when they are over-parameterized. 

While discussing statistical learning (before deep learning) with Peter Norvig, Chomsky noted that ``we cannot seriously propose that a child learns the values of $10^9$ parameters in a childhood lasting only $10^8$ seconds.'' One has to wonder if a similar argument applies to biological neurons: humans have $80$ billion neurons, each with thousands of synapses. If childhood is only $10^8$ seconds, how do all the connections get set? Well, also note that the $3$ billion base pairs of the human genome certainly can't specify the precise connections either. Something must be wrong with the argument. 

Two missteps are easy to spot. First, even if a model has billions of parameters, they will not generally be independent. This means that a single data point could set or move thousands or millions or billions of parameters. For example, observing a single sentence with SVO order might increase (perhaps millions of) parameters that put S before V, and decrease (perhaps millions of) parameters that put S after V. Steps of backpropagation don't change one parameter -- they change potentially \textit{all} of them based on the locally best direction (the gradient). 

Second, these models, or learners, often don't need to pinpoint exactly one answer. A conjecture called the \textit{lottery ticket hypothesis} holds that the behavior of a deep learning model tends to be determined by a relatively small number of its neurons \citep{frankle2018lottery}. Thus, the massive number of parameters is not because they all need to be set exactly to some value. Instead, having many degrees of freedom probably helps these models learn well by giving the models directions they can move in to avoid getting stuck. It may be like how it is easier to solve a puzzle if you can pick the pieces up and put them down (move them in a third dimension) rather than just sliding them around the table. More degrees of freedom can \textit{help} configure your theory to work well. 

\subsection{Hierarchical structure need not be innate}

These models discover structure -- including hierarchical structure -- from their training \citep{hewitt2019structural,manning2020emergent,lin2019open,tenney2019you,warstadt2020can,kim2020pre,linzen2021syntactic,mahowald2023dissociating}. These models certainly could learn rules based on linear, rather than hierarchical, structure, but the data strongly leads them towards hierarchical, structural generalization. This capacity to discover hierarchical structure rather than building it in was long-emphasized by cognitive psychologists, for instance work inducing syntactic categories by clustering \citep{redington1998distributional,clark2000inducing}, or \citet{perfors2011learnability}, who showed that a Bayesian model comparison applied to child-directed speech could identify a context-free (hierarchical) grammar over alternatives.  The most recent models build on earlier efforts to encode hierarchical processes into neural networks (e.g. \citealt{tabor1997parsing,christiansen1999toward,tabor2000fractal,smolensky2006harmonic}).  

Chomsky's familiar example of aux-inversion is meant to show that children must possess hierarchical syntax. As a simple experiment, we can also ask these models to form some questions. For example, the familiar aux-inversion:

\begin{myquote}
\textit{Convert the following into a single question asking if the accordion is in the rain: ``The accordion that is being repaired is out in the rain.''}\medskip\\
\noindent Is the accordion that is being repaired out in the rain?
\end{myquote}

\noindent More rigorous experiments demonstrating aux-inversion success even in earlier language models can be found in \citew{warstadt2020can}. It is philosophically interesting that this model knows which ``is'' to ``move'', but I think it's staggering that unlike pretty much every other model in the whole of science, we can interrogate these models simply by \textit{asking} them to perform a certain behavior. It's also interesting to ask it to generate a few more examples, without giving guidance on the nature of the question:

\begin{myquote}
\textit{Form the following sentence into a question in ten different ways: ``The accordion that is being repaired is out in the rain.''}\medskip\\
\noindent Is the accordion that is being repaired out in the rain?\\
Where is the accordion that is being repaired located in the rain?\\
In what condition is the accordion being repaired out in the rain?\\
Is the accordion being repaired, which is outside, getting wet in the rain?\\
Is the accordion being repaired, which is out in the rain, being protected from the water?\\
Is the accordion being repaired out in the rain, and if so, will it be damaged by the water?\\
How long has the accordion that is being repaired been out in the rain?\\
Is the accordion, which is being repaired, being kept in the rain outside?\\
What is happening to the accordion, which is being repaired, while it is out in the rain?\\
Is the accordion that is currently being repaired exposed to the rain outside?
\end{myquote}

\noindent It is remarkable too that the models maintain enough representation of its prior discourse to generate distinct questions. The responses also highlight integration of syntax and semantics, for instance knowing that such an accordion might be ``protected from the water'' or ``damaged by the water''. 

The aux-inversion cases have been interesting only insofar as interrogatives are derived from declaratives. The question of how kids know which ``is'' to move is nonsensical outside of that assumption. These models, which are trained only on text prediction, provide an implemented account in which we don't need to think of interrogatives as derived from declaratives (it seems very unlikely this is what is happening inside the model). The models might thus lead us to consider connections between other constructions, such as those used in the above list, which are likely located nearby to the target question in the model's latent space of activations. For optimally predictive theories, constructions may be connected to each other but not in the way generative theories of syntax predict.
   

\subsection{Language and thought dissociate}

Human language, for Chomsky, is deeply interconnected to human thought \citep{everaert2015structures}. \citet{chomsky2002nature} describes language as ``a system for expressing thought;'' in fact, one which is used primarily for speaking to oneself. Interestingly, he does not draw on the literature on inner monologues, which show substantial variation between individuals, with some describing virtually no internal language use at all (e.g. \citealt{reed1916existence,heavey2008phenomena,roebuck2020internal}). Chomsky's view, though, is made perhaps more plausible by arguments that thought itself shares many properties of language, namely a compositional, language-like structure \citep{fodor1975language,fodor1988connectionism,goodman2014concepts,piantadosi2016four,quilty2022best}. Chomsky frequently contrasts his inner thought view of language with the idea that language primarily is structured to support communication (e.g. \citealt{hockett1959animal,bates1982functionalist,gibson2019efficiency}), although it's worth noting he sometimes draws the opposite predictions from what efficient communication would actually predict (e.g. \citealt{piantadosi2012communicative}). \citet{mahowald2023dissociating} argue in a comprehensive review that large language models exhibit a compelling dissociation between linguistic ability and thinking. The models know so much syntax, and aspects of semantics, but it is not hard to trip them up with appropriate logical reasoning tasks. Thus, large language models provide a proof of principle that syntax can exist and likely be acquired separately from other more robust forms of thinking and reasoning. Virtually all of the structure we see in language can come from learning a good model of strings, not directly modeling the world. 

Models therefore show a logically possible dissociation between language and thinking. But a considerable amount of neuropsychological evidence supports the idea that language and thought are actually separate in people as well. \citet{fedorenko2016language} review a large patient literature showing that patients with aphasia are often able to succeed in tasks requiring reasoning, logic, theory of mind, mathematics, music, navigation, and more. Aphasic patient studies provide an in vivo dissociation between language and other rational thinking processes. They also review neuroimaging work by \citet{fedorenko2011functional} and others showing that the brain regions involved in language tend to be \textit{specific to language} when it is compared to other non-linguistic tasks. That is not what would be predicted under theories where language is inherently tied to thought. 

This is not to say that there is no way language and thought are related -- we are able to specify some kinds of reasoning problems, communicate solutions, and sometimes solve problems with language itself. A compelling proposal is that language may be a system for connecting other core domains of representation and reasoning \citep{spelke2003makes}.

\section{Why this and not that}\largerpage

Chomsky maintains (in the same Marcus interview above) that large language models have achieved nothing because they fail to explain  ``Why this?  Why not that?'' The question of whether these models can explain why human language has the form that it does is an interesting one that likely depends on whether the language system evolved before language or concurrently with it. If language co-opted neural systems for general sequential prediction (e.g. \citealt{christiansen2015language}), it's possible we had some architecture like these models before we had language, and therefore the form of language \textit{is} explained by the pre-existing computational architecture. On the other hand, if the two co-evolved, language might not be explained by the processing mechanisms. With such uncertainty,  there are ``why'' questions that a large language model may not answer. This does not mean the models lack scientific value. In the same way, Newton's laws don't answer why those are the laws as opposed to any other, and yet they still embody deep scientific insights. Anyone who has had a child ask ``why'' repeatedly will recognize that at some point, \textit{everyone}'s answers ground out in assumption. 

However, it is worth highlighting in this context that Chomsky's own theories don't permit particularly deep ``why'' questions either. In large part, he simply states that the answer is genetics or simplicity or ``perfection'', without providing any independent justification for these claims.\footnote{Indeed, as \citet{johnson1997critique} argue, the assumptions lead to considerable complexity -- e.g. ``by building a global economy  metric into syntax to identify an optimal derivation, the [Minimalist Program] introduces considerable conceptual and computational complexity into linguistic theory, and so produces a model of grammar that is far less economical than those that employ only constraints on representations \ldots''} For example, readers of \citew{berwick2016only} -- a book titled \citetitle{berwick2016only} -- might have hoped to find a thorough and satisfying ``why'' explanation. Their answer boils down to people having \textit{merge} (essentially chunking two elements into one, unordered). And when it comes down to explaining \textit{why} merge, they fall down the stairs: they simply state that ``merge'' is the minimal computational operation, apparently because that's what they think and that's that. Forget the relativity of definitions of simplicity, articulated by \citet{goodman1965new}, where what is considered simple must ground out in some convention. Berwick \& Chomsky do not even attempt to explain why they believe ``merge'' is simpler than other simple computational bases, like cellular automata or combinatory logic or systems of colliding Newtonian particles -- all of which are capable of universal computation (and thus encoding structures, including hierarchical ones). Or maybe more directly, what makes \textit{merge} ``simpler'' or more ``perfect'' than, say, backpropagation? Or \citegen{elman1996rethinking} architectural biases? Berwick \& Chomsky don't consider these questions, even though the ability to scientifically go after such ``why'' questions is supposed to be the hallmark of the approach. One might equally just declare that a transformer architecture is the ``minimal'' computational system that can handle the dependencies and structures of natural language and be done with it.

We should not actually take it for granted that generative syntax has found any regularities across languages that need ``why'' explanations. \citet{evans2009myth} have made a convincing empirical case that prior features hypothesized to be universal~-- and thus plausibly part of the innate endowment of language~-- actually are not found in \textit{all} languages. Perhaps most damningly, not even all languages appear to be recursive \citep{everett2005cultural,futrell2016corpus,pullum2023dan}, contradicting \textit{the} key universality claim from \citew{hauser2002faculty}. \citet{dkabrowska2015exactly} highlights profound differences in adult grammars between languages and the lack of a coherent formulation from generative linguists.  None of this is to say that we won't be able to find any universals; rather, the proposed ones aren't there, and the differences between languages may be more scientifically informative than their commonalities \citep{pullum2009universals}. On the methodological side, statistical analyses show that in order to justifiably claim something as universal, one would need on the order of $500$ \textit{statistically independent} languages, which is likely beyond what is currently in existence \citep{piantadosi2014quantitative}. 

However, the question of why languages are the way they are does have plausible, testable hypotheses. These hypotheses are interdisciplinary \citep{reali2009necessity} and include variegated influences of communicative \citep{zipf1965psycho,hockett1959animal,bates1982functionalist,piantadosi2012communicative,gibson2013rational,gibson2019efficiency,coupe2019different,hahn2020universals,futrell2022information}, cultural \citep{everett2005cultural,lupyan2010language,dale2012understanding,everett2015climate}, ecological \citep{lupyan2016there}, learning \citep{smith2012compositionality,steinert2019learnability,steinert2020ease}, and cognitive factors \citep{gibson2000dependency,futrell2015large}. Such pressures have led to efficient and useful properties, including minimization of dependency structures \citep{futrell2015large}, the presence of communicatively useful ambiguity \citep{piantadosi2012communicative}, and efficiency in lexical systems \citep{kemp2012kinship,kemp2018semantic,zaslavsky2019semantic,steinert2020quantifiers,mollica2021forms,mahowald2022efficient,denic2022indefinite}. Recent advances in understanding cultural evolution also should shape our theories of linguistic nativism, highlighting that weak biases are often sufficient over the course of cultural transmission to lead to stable patterns across languages \citep{thompson2016culture,kirby2014iterated,chater2009restrictions}. All of these are factors that Chomsky and others have never really grappled with, much less successfully ruled out as alternative answers to the ``whys'' of language.  

\section{The refutation of method}

Chomsky often describes his own approach as ``Galilean'', meaning that he seeks the underlying principles in phenomena rather than analysis of large amounts of data. The term is both a misnomer \citep{behme2014galilean} and a not-so-subtle insult to colleagues who choose to work from different assumptions. Of course,  Galileo cared about quantitative measurement of the world in order to formulate theories, developing tools of his own and even trying to measure the speed of light.\footnote{``Measure what can be measured, and make measurable what cannot be measured.''} Chomsky's view was clearly articulated in an interview with Yarden Katz in 2012 where, at the time, he was focused on explaining that Bayesian models were useless \citep{katz2012noam}: 
\begin{quote}
 \ldots{}  [S]uppose that somebody says he wants to eliminate the physics department and do it the right way. The ``right'' way is to take endless numbers of videotapes of what's happening outside the [window], and feed them into the biggest and fastest computer, gigabytes of data, and do complex statistical analysis -- you know, Bayesian this and that -- and you'll get some kind of prediction about what's gonna happen outside the window next. In fact, you get a much better prediction than the physics department will ever give. Well, if success is defined as getting a fair approximation to a mass of chaotic unanalyzed data, then it's way better to do it this way than to do it the way the physicists do, you know, no thought experiments about frictionless planes and so on and so forth. But you won't get the kind of understanding that the sciences have always been aimed at~– what you'll get at is an approximation to what's happening.
\end{quote}
It's worth pinpointing exactly where this kind of thinking has gone wrong because it is central to the field's confusion in thinking about large language models. Chomsky's view certainly does not address the above idea that parameter fitting in a statistical model often \textit{is} theory building and comparison. 

But another factor is missing, too. Over modern scientific history, many computational scientists have noticed phenomena of \textit{emergence} \citep{goldstein1999emergence,mcclelland2010emergence}, where the behavior of a system seems somewhat different than might be expected from mere knowlege of its underlying rules. This idea has been examined specifically in language models \citep{wei2022emergent,manning2020emergent}, but the classic examples are older. The stock market is unpredictable even when individual traders might follow simple rules (``maximize profits''). Market booms and busts are the emergent result of millions of aggregate decisions. The high-level phenomena would be hard to intuit, even with full knowledge of traders' strategies or local goals. The field of complex systems has documented emergent phenomena virtually everywhere, from social dynamics to neurons to quasicrystals to honeybee group decisions. The field to have most directly grappled with emergence is physics, where it is acknowledged that physical systems can be understood on multiple levels of organization, and that the same laws that apply one one level (like molecular chemistry) may have consequences that are difficult to foresee on another (like protein folding) \citep{anderson1972more,crutchfield1994anything,crutchfield1994calculi}. 

Often, the only way to study such complex systems is through simulation. We often can't intuit the outcome of an underlying set of rules, but computational tools allow us to simulate and just \textit{see} what happens. Critically, simulations \textit{test the underlying assumptions and principles} in the model: if we simulate traders and don't see high-level statistics of the stock market, we are sure to have missed some key principles; if we model individual decision-making for honeybees but don't see emergent hive decisions about where to forage or when to swarm, we are sure to have missed principles. We don't get a direct test of principles because the systems are too complex. We only get to principles by seeing if the simulations recapitulate the same high-level properties of the system we're interested in. And in fact the \textit{surprisingness} of large language models' behavior illustrates how we don't have good intuitions about language learning systems. 

We can contrast understanding emergence through simulation with Chomsky's attempt to state principles and reason informally (see \citealt{pullum1989formal}) to their consequences. The result is pages and pages of stipulations and principles (see, e.g., \citealt{collins2016formalization} or \citealt{chomsky1995minimalist}) that nobody could look at and conclude were justified through rigorous comparison to alternatives. After all they \textit{weren't}: the failure of the method to compare vastly different sets of starting assumptions, including neural networks, is part of why modern large language models have taken everyone by surprise. The fact that after half a century of grammatical theories, there can be a novel approach which so completely blows generative grammar out of the water on every dimension is, itself, a refutation of the ``Galilean method''.

An effective research program into language would have considered, perhaps even developed, these kinds of models, and sought to compare principles like those of minimalism to the principles that govern neural networks. This turn of events highlights how much the dogma of being ``Galilean'' has counterproductively narrowed and restricted the space of theories under consideration -- a salient irony given Chomsky's (appropriate) panning of Skinner for doing just that.\footnote{``What is so surprising is the particular limitations [Skinner] has imposed on the way in which the observables of behavior are to be studied.'' \citep{chomsky1959chomsky}} 

\section{Enduring contributions of Chomsky's program}

I have attempted to convey my own sense of excitement about large language models, as well as my own pessimism about several aspects of Chomsky's approach to linguistics. However, it is easy to see that beyond the critiques above, many of Chomsky's emphatic focuses will survive his specific theories. For example, one of Chomsky's lasting contributions to cognitive science will be his emphasis on the reality of cognitive structure, like Tolman, Newell \& Simon, Miller, and others of the cognitive revolution \citep{nadel2003cognitive,margaret2008mind}. The search for the properties of human cognition that permit successful language acquisition is clearly central to understanding not just the functioning of the mind, but understanding humanity. It is a deep and important idea to try to characterize what computations are required for language, and to view them as genuinely mental computations. Chomsky's focus on children as creators of language, and on understanding the way in which their biases shape learning is fundamental to any scientific theory of cognition. Linguistic work in Chomsky's tradition has done a considerable amount to document and support less widely spoken languages, a struggle for current machine learning \citep{blasi2021systematic}. The overall search for ``why'' questions is undoubtedly core to the field, even if there are disagreements about what counts as an answer.

Some of the ideas of Chomsky's approach are likely to be found even in language models. For example, the idea that many languages are hierarchical is likely to be correct, embodied in some way in the connections and links of neural networks that perform well at word prediction.  There may be a real sense in which other principles linguists have considered are present in some form in such models. If the models correctly perform on binding questions, they may have some computations similar to binding principles. But none of these principles needed to be innate. And in neural networks, they are realized in a form nobody to date has written -- they are distributed through a large pattern of continuous, gradient connections. Moreover, the representation of something like binding is extraordinarily unlikely to have the form generative syntax predicts since the required underlying representational assumptions of that approach (e.g. binary branching, particular derivational structures, etc) are not met. 

Another key contribution of Chomsky's research program has been to encourage discovery of interesting classes of sentences, often through others like \citet{ross1967constraints}. Regardless of the field's divergent views on the reality of WH-movement, for example, the question of \textit{what} determines grammaticality and ungrammaticality for WH-sentences is an important one. Similarly,  phenomena like ``islands'' do not go away because of large language models -- they are targets to be explained (and models do a pretty good job according to analyses by \citealt{wilcox2022using}). Such phenomena are often difficult to separate from theory, as in the example above about whether declaratives and interrogatives are actually connected in the real grammar. Regardless of theory, researchers working in Chomsky's tradition have illuminated many places where human linguistic behavior is more complicated or intricate than one might otherwise expect.

As articulated by \citet{pater2019generative}, the field should seek ways to integrate linguistics with modern machine learning, including neural networks. I have highlighted some researchers whose approach to language clearly resonates with the insights of modern language models. The current upheaval indicates that we should foster a pluralistic linguistics that approaches the problem of language with as few preconceptions as possible -- perhaps even a fundamental reconceptualization of what language is for and what it is like \citep{edelman2019verbal}. Maybe many of the ``syntactic'' phenomena that Chomskyan theories have concerned themselves with are really about something else, like pragmatics or memorized constructions. Maybe the universals of language -- if there are any -- come from aspects of use like communicative and cognitive pressures, or other cultural factors. Maybe linguistics could learn from the methods of cognitive science \citep{edelman2007bridging}. Maybe theories of grammar should respect humans' unparalleled memory capacity for sequential material. Maybe we should have linguistics students learn information theory, probability, neural networks, machine learning, anthropology, numerical methods, model comparison, Kolmogorov complexity, cognitive psychology, language processing, multi-agent systems, etc. The most permanent legacy of Chomsky's approach could be as an admonishment about what happens when the study of language is separated from the rest of science. 


\section{Conclusion}

One must be frank about the state of the art for models that capture syntax. It's not that large language models offer slightly higher performance than other approaches in linguistics; it's not that they better cover some corners of syntax. It's that there is \textit{nothing comparable in all of linguistic theory} to the power of large language models in both syntax and semantics -- much less discourse coherence, style, pragmatics, translation, meta-linguistic awareness, non-linguistic tasks, etc. They are game changers on all fronts. Optimists who view them as at least a plausible direction for acquisition see them as a way to build in and test architectural principles and biases, long emphasized by connectionist modelers like \citet{mcclelland1986parallel}, \citet{elman1996rethinking}, \citet{smolensky2006harmonic}, and others. Those who doubt they could function as models of acquisition should nonetheless see the success of gradient representations, architectural assumptions, and implicit or emergent principles as a theory of grammar. These models have opened the space of plausible linguistic theories, allowing us to test principles beyond the ones that have traditionally concerned linguists. They allow us to finally develop compelling theories of the interplay of structure and statistics. And they appear to solve many problems that generative syntacticians have worried about, but without using any of their theoretical tools and constructs. Large language models rewrite the philosophy of approaches to language. 

It's worth ending with a reminder of how impressive these models are:

\begin{myquote}
\textit{Explain how large language models undermine Chomsky's main claims about linguistics.}\medskip\\
\noindent Noam Chomsky is a prominent linguist and philosopher who has made significant contributions to the field of linguistics and language theory. He has put forward the idea that language is an innate, biologically determined ability that is unique to humans, and that all humans possess a universal grammar, or a set of innate linguistic rules that allow us to understand and produce language.

However, the development of large language models like GPT-3 has challenged some of Chomsky's main claims about linguistics and the nature of language.

First, the fact that language models can be trained on large amounts of text data and can generate human-like language without any explicit instruction on grammar or syntax suggests that language may not be as biologically determined as Chomsky has claimed. Instead, it suggests that language may be learned and developed through exposure to language and interactions with others.

Second, the success of large language models in performing various language tasks such as translation, summarization, and question answering, has challenged Chomsky's idea that language is based on a set of innate rules. Instead, it suggests that language is a learned and adaptive system that can be modeled and improved through machine learning algorithms.

Finally, the ability of language models to generate coherent and coherent [sic] language on a wide range of topics, despite never having seen these topics before, suggests that language may not be as rule-based as Chomsky has claimed. Instead, it may be more probabilistic and context-dependent, relying on patterns and associations learned from the text data it was trained on.

In conclusion, while Chomsky's contributions to the field of linguistics have been significant, the development of large language models has challenged some of his main claims and has opened up new avenues for exploring the nature of language and its relationship with machine learning and artificial intelligence.
\end{myquote}

\noindent Models that can do this are a gift. They make language the most exciting arena in all of cognitive science and AI. Such models are also a tool that will help linguistics to refine theories and compare leading ideas to strong alternatives.

It's worth remembering that Chomsky's approach to linguistics isn't so much about his particular beliefs -- many linguists who work in his tradition disagree with him on specific points. And there is nothing wrong with being wrong: Chomsky's own theories elegantly articulated many deep, compelling ideas that have motivated linguistic and cognitive research. But modern language models highlight the weaknesses of his methodological prescriptions. One cannot seek principles without justifying them with rigorous experiments and comparisons; one can't reason about complex systems without implementations; one can't discover mechanisms without formally testing completely different approaches (like neural networks); one can't proclaim simplicity, optimality, or ``perfection'' without seriously connecting these terms to formal notions. These arguments worked on many linguists and consequently prevented that tradition from developing anything close to the wild successes of modern language models.

Frederick Jelinek's quip ``Every time I fire a linguist, the performance of the speech recognizer goes up'' \citep{jelinek1988applying} was a joke among linguists and computer scientists for decades. I've even seen it celebrated by academic linguists who think it elevates their abstract enterprise over and above the dirty details of implementation and engineering. But, while generative syntacticians insulated themselves from implementation, empirical tests, and formal comparisons, engineering took over. And now, engineering has solved the very problems the field has fixated on -- or is about to very soon. The unmatched success of an approach based on probability, internalization of constructions in corpora, gradient methods, and neural networks is, in the end, humiliating for a subfield of linguistics that has spent decades deriding these tools. But now we can do better. 


\section*{Acknowledgements}

I am grateful to Benjamin Hayden, Ev Fedorenko, Geoffrey Pullum, Kyle Mahowald, Shimon Edelman, Stefan Müller, Bob Levine, and Dan Everett, for detailed comments and suggestions on this paper. This paper benefited greatly from Felix Hill's Twitter presence (@FelixHill84), especially on topics like emergence and earlier connectionist work. 


\section*{Postscript: A commentary on replies}

The original draft of this paper, posted on LingBuzz, received a number of replies from authors, including those who primarily work in generative theories of linguistics. Many offered a critical appraisal of my claims and in the summer of 2023, I added this postscript  summarizing their critiques and giving brief responses.

\citet{rawski2023modern} wrote a single page response titled \citetitle{rawski2023modern}. They say the argument I made is invalid in that I argued that models which correlate with behavior (and neural data) are good candidates for what people actually do, or at least promising directions. They say that my argument is not \textit{deductively} valid since it ``fallaciously affirms the consequent''. This logical error is the one you make if you know that swimming causes you to get wet ($p \to q$), you see someone wet ($q$) and claim to know that they went swimming ($p$). They, of course, could be wet from showering or rain or something else, so you did not draw a valid conclusion. In our case, a model showing the same behavior as people ($q$) does not logically entail that it works in the same way ($p$), even though we know if it worked the same way it would show the same behavior ($p \to q$). Sure, great, I agree it's not a \textit{deductive} argument. But very little in science is. Scientific theories are more often \textit{inductive} and take the form I describe: we think theories of physics or biology are likely because they predict the experimental data, not because they are deduced as some logical consequence of the experimental data. This setup can be formalized in e.g. Bayesian inference, and is common in Bayesian data analysis, machine learning, and Bayesian philosophy of science (see, e.g., \citealt{jeffreys1998theory,godfrey2009theory}). From this point of view, \citegen{rawski2023modern} attempt to cast science as deduction is as strange to me as saying, ``Even though the equations of thermodynamics did a good job of predicting what happened when I made a steam engine, it's fallaciously affirming the consequent to therefore increase my belief that thermodynamics is a true theory of the physical world.'' 

\citet{rawski2023modern} also disliked my analogy to forces ($1/r$ compared to $1/r^2$) and brought up the historical example of how a heliocentric theory of the solar system made empirically worse predictions than a geocentric one. They are correct that there are other considerations in scientific theories, such as simplicity and explanation. Simplicity is very often formalized with e.g. Bayesian methods (that are not popular in generative linguistics by any means) and simplicity is often a consideration in neural networks and machine learning, which include regularization or other architectural features. However, \citet{rawski2023modern} seem to misunderstand the linkage between experiment and theory: they argue that ``explanatory power, not predictive adequacy, is directly responsible'' for essentially everything in physics. Not all physicists think a strong focus on non-empirical considerations has been good for physics \citep{hossenfelder2018lost}. But a key ingredient \citet{rawski2023modern} leave out of the success story of modern physics is \textit{data}: physics actually tests its theory against alternatives~-- in fact, pretty much any alternative anyone can think of. Physics even works to actively chart out the landscape of mathematically \textit{possible} theories. Modern physics actually rigorously implements a ``standard model'' that generates quantitative predictions, which it constantly attempts to falsify with new experimental work. Physics spent $\$9$ billion on the Large Hadron Collider, and $\$10$ billion on the James Webb Space Telescope in order to gather \textit{empirical data and compare quantified, implemented theories}. As I pointed out, generative syntax didn't even enter SyntaxGym. 

\citet{milway2023response} wrote a brief response that first argues that the title of my paper is a ``category error'' since ``one cannot refute an approach.'' I'd prefer not to be prescriptive about lexical semantics. There are plenty of papers for example talking about a ``refutation of behaviorism'' (many attributing that to Chomsky!), ``refutation of psychologism'', or a ``refutation of Copernicanism,'' etc. Refuting an approach means showing how its core assumptions probably don't work, and therefore that the approach is unlikely to be productive. My paper detailed which core assumptions I thought would not work and why. 

\citet{milway2023response} also did not like the generated examples for ``Colorless green ideas sleep furiously''; nor did Gary Marcus, who critiqued these particular responses in a public video but then backed out of a public discussion with me about the paper. Milway and Marcus were unimpressed because the model's generated sentences (e.g. ``Black shiny kangaroos hop playfully'') still had meaningful pairs of words (a kangaroo, allegedly, could be ``shiny'' but an idea certainly can't be ``green''). I, on the other hand, was impressed that the model knew enough to generate the same \textit{syntactic} form, which was evidence that it knew some syntax, and in fact could separate it from semantics. It was strange to me that authors criticized these responses as showing the models do not exhibit ``human-like behavior'' without even wondering whether people would produce meaningless bigrams from the same prompt.

\citet{milway2023response}, \citet{kodner2023linguistics}, and others emphasized that a problem with modern language models is that you do not know \textit{why} they show the behavior that they do. Here is Milway:
\begin{quote}
[Piantadosi] presents ChatGPT data showing grammatical aux-inversion in English, but provides no explanation as to how it achieves this. Such an explanation though, is at the core of Chomsky’s approach to language. If MLMs do not provide an explanation, then how can they supplant Chomsky’s approach?
\end{quote}
It is correct that we do not, at present, know how the models achieve this and my claim was never that our work was done. The argument I made was that (i) these models do it \textit{somehow} and (ii) how they do it is almost certainly different from Chomsky's approach, and (iii) their approach works \textit{really, really} well. However they do it, they can actually produce streams of grammatical sentences! And translate, and summarize, and elaborate, etc. Whatever principles allow models to do these things are the most promising principles for modern linguistics. I specifically argued that they were likely things like reliance on probability, memorized constructions, implementations of lexical semantics, etc. 

I'll agree, though, that there is an interesting debate about the nature of science lurking here. The critics' position seems to be that in order for something to be a scientific theory, it must be \textit{intuitively comprehensible} to us. I disagree because there are many phenomena in nature which probably will \textit{never} admit a simple enough description for us to comprehend. We cannot just exclude these things from scientific inquiry. There probably is no simple theory of a stock market (why IBM takes on a particular value) or dynamics in complex systems (why an $O_2$ molecule hits a particular place on my eyeball). Certainly there are local, proximate causes (Tom Jones bid $\$142$ for IBM; the $O_2$ molecule was bumped by another), but when you start to trace these causes back into the complex system, you will quickly exceed our ability to understand the complex network of interactions. Language \textit{might} just be like that. Other scientific theories certainly seem just outside of what our intuitive cognitive systems can handle, and yet we discover and use them. Quantum mechanics is a nice example: we find it highly counterintuitve ``why'' or ``how'' something acts both like a particle and a wave -- impossible to intuit, but eminently scientific. Physics sometimes adopts a ``shut up and calculate'' mentality \citep{david1989s} to push back against the angst of failing to find good intuitive explanations for the theory. That doesn't stop the field from doing science. 

But on this point, none of the authors responded to my comments on the shallowness of those explanations that are supposed to be so central to Chomsky's approach. There are \textit{no} theories of how Principle C or A-chains or whatever are encoded in human brains, much less genomes. So while some bask in the glory of explanations based on those pieces, others who want one little step of further explanation will be left empty-handed. In my own view, that is too fragile a kind of explanation to use as the knock-down advantage of the approach. Worse, there are now quite a few examples where the explanations provided by Chomsky's approach probably aren't even in the right ballpark. For example, ``island constraints'' in syntax seem very well explained by other factors like frequency, memory, discourse, or pragmatics \citep{kluender1992deriving,kluender1993subjacency,kehler2002coherence,chaves2020unbounded,goldberg2006constructions,liu2022structural,winckel2021french,ambridge2008island,abeille2020extraction,liu2022structural,liu2022verb,cuneo2023discourse}. This stands in contrast to the elaborate machinery constructed by generative syntax. If the explanations provided are wrong, it makes it hard to argue they were a key, defining strength of the approach. Some of us would prefer rigorous comparison between theories with very different starting assumptions, rather than celebration of merely theory-internal chains of ``explanation''.

Several authors have also pointed out that modern language models can learn ``impossible languages'' \citep{milway2023response,moro2023large}, although nobody seems to think it's important to first know how much data these architectures (or nearby ones) would require in order to learn ``possible'' versus ``impossible'' languages. There is historical irony to this move because, without missing a beat, the field switched from saying that innateness was true because learning was impossible (``poverty of stimulus''), to saying that learning can't be right because it works too well. But also, simultaneously somehow, we should doubt the models since they don't work  on small amounts of data? A related tangle from the ``abundance of the stimulus'' has also been around in generative circles for a while \citep{babyonyshev2001maturation}, essentially saying that slow learning of frequent constructions is evidence for innateness too.  Of course, nobody says what timing for learning would be evidence \textit{against} innateness. 

\largerpage
Even if the right model has constraints enforcing some typology, as generative syntax claims, that does not imply that the best scientific method will be to include these constraints from the start. If you want to figure out how to juggle knives -- a constraint not to touch the blades -- you might do well to start by juggling balls (no constraints) really well. Similarly, approaches to constrained optimization often care quite a bit about the unconstrained solution. The reason is that if you ignore constraints, you can often come up with approximately right solutions, which can then be refined. If you try to take constraints into account too early, you might have a harder time discovering the key pieces and dynamics, and could create a worse overall solution. For language specifically, what needs to be built in innately to explain the typology will interact in rich and complex ways with what can be learned, and what other pressures (e.g. communicative, social) shape the form of language. If we see a pattern and assume it is innate from the start, we may never discover these other forces because we will, mistakenly, think innateness explained everything. 


More troubling, the idea of ``impossible languages'' has never actually been empirically justified. Nobody knows what the space of possible languages is. There are no examples where universals have been shown to be due to something specific to language through rigorous comparison to non-linguistic domains. And many hypotheses turn out to be wrong. The example of recursion and Pirah\~{a} is probably the most striking case of a universal that wasn't. \citet{pullum2023dan} points to a number of languages which were argued not to have recursive embedding, even \textit{before} \citet{hauser2002faculty}'s claims that recursion was distinctly human and universal. I pointed to compelling reviews that very little which has claimed to be universal actually is \citep{evans2009myth}, and that the claims are not sufficiently justified statistically \citep{piantadosi2014quantitative}. 

\largerpage
\citet{katzir2023large} does name a few specific universals, but the examples do not make a good case for innateness. One is about WH-questions, and I pointed above to non-syntactic theories of these. One is that there are no palindrome-like phonological sequences. Other hypotheses about the absence of palindromes -- likely facts about how memory or sequential processing works -- are plausible, and the field should care about comparing these alternatives. I am not aware of work on exactly the right comparison (mirror vs. not), but mirror-repetitions sequences are possible for kids and monkeys to learn, though monkeys show substantial difficulty especially compared to kids \citep{jiang2018production}. So if the claim is that those kinds of patterns are genetically impossible, that's probably not true; if the claim was that something linguistic, as opposed to more generally cognitive, made them hard, that's probably not the right explanation either. \citet{katzir2023large} contrasts palindromes with hierarchical processes, as another claimed part of innate syntax, but there is evidence for hierarchical behavior in other primates \citep{ferrigno2020recursive,voloh2023hierarchical} and even crows \citep{liao2022recursive}, making hierarchy unlikely to be about innate syntax. \citet{katzir2023large} points to conservativity of quantifiers as another universal, but the child learning studies of these quantifiers provide mixed results \citep{spenader2019conservative}, and many properties of quantifiers, including conservativity and monotonicity, can be derived from more general principles \citep{steinert2019learnability,carcassi2021monotone,van2021quantifiers,steinert2021quantifiers}.
The assertion that any of these phenomena are known to be innate linguistic universals is an Olympic triple jump over a contested set of hypotheses. 

Several authors also point to places where some implementations of language models show non-human-like judgments of acceptability. It is useful to find examples that models examples mess up, but it really can't serve as a form of \textit{comparison} between theories. There are, also, plenty of tasks where language models do \textit{better} than anything using theories based on generative grammar, like translation, summarizing, and interpretation. Single sentences can't serve as a comparison of theories until we are specific on (i) what the important set of phenomena are, and (ii) how specific implementations of e.g. Chomsky's minimalism do on the corresponding sentences. Part of my complaint was that generative linguists don't typically have implementations and don't quantitatively compare theories, so sentences they fail on are just anecdotes.


Moreover, failures on examples at this point may not mean much since the approach of modern neural networks is so new. It very well could be that closely related architectures will succeed on these sentences. \citet{kodner2023linguistics} tries to say that the architectural changes we build into the next round of neural networks \textit{are} Chomsky's universal grammar (``But what are these biases, principles, and limitations [we can build into neural networks] if not some form of the  Universal Grammar?''). But I think that's deeply mistaken. There are no doubt \textit{some} principles required for language. The question is whether they are language-specific (or syntax-specific), innate, and whether they have the form that Chomsky and similar theorists have said, as opposed to lower-level principles that work through emergence. I am not sure that anyone could claim with a straight face that Chomsky's theory of Universal Grammar was right all along because one neural network (e.g. transformers) works better than another (e.g. LSTMs). 


Several authors also pointed to differences in the amount of data between current models and children's input. It is certainly true that modern language models are currently trained on more data than children receive. But Chomskyan authors write about this as though the models are known to \textit{need} that much data for syntax specifically. That seems very unlikely to be true. No authors who highlighted this point about data even engaged the hypothesis that most of this data is probably for semantics rather than syntax. If it's hard for large language models to learn semantics from text, that wouldn't tell us much about how real language acquisition works because children independently receive all kinds of other information sources about meaning. AI companies use as much data as they can find because it seems to help models on the rich kinds of tasks that they want to solve, often involving reasoning and complex discourse or conversation. But if we were only concerned with capturing syntax, it may be that much less data is required. A nice picture of learning some sequential regularities from hundreds of thousands of words was published this past summer in the NY Times,\footnote{\textit{Let us show you how GPT works -- Using Jane Austen} by  Aatish Bhatia.} but the BabyLM challenge should tell us a lot more. Until the space of architectures is well-explored, the question of how far into syntax these kinds of models can get is entirely open. 


\citet{kodner2023linguistics} also disputed my claim that language models work over an ``unconstrained space''. A better term might have been ``relatively unconstrained'' because there are constraints and priors in any learning system. The point is that those priors do not seem to be very strong or limiting (which is precisely why others complained about them learning ``impossible'' languages). They also need not be stated symbolically, as they are in Chomskyan theories. \citet{kodner2023linguistics} erroneously connect the lack of constraints to the amount of data, claiming that ``every'' algorithm which learns over a broad (computable) class of languages ``requires infeasible amounts of time and data.'' This is a strange critique because my lab recently showed in \citet{yang2022one} that tiny amounts of data could suffice for learning key computations needed in grammar out of a huge space of possible algorithms -- and Kodner led a reply to the paper,  although he incorrectly summarized the basic framework our model uses \citep{piantadosi2022reply}. It is not true that large hypothesis spaces always require huge amounts of data, in part because data points can be very informative: each single bit of information in the data can cut the space of likely hypotheses in half. 

 
Overall, I was disappointed in some aspects of the responses: nobody said much about how this technology \textit{should} affect our linguistic theories. As a result, most of the points that these authors raised cut orthogonally to the key claims in my paper.  Few responded to what I see as the primary failings of Chomskyan linguistics, including aspects like avoidance of probability and constructions, informality, failure to compare to different starting points, incorrect learnability claims, shallowness of ``why'' answers, failures of general predictions about  typology, lack of clearly articulated and justified universals, lack of quantitative comparison to alternative models even on syntactic phenomena of interest, etc. These are issues that linguists who work in that tradition can fix. 

I am optimistic, though, about the future of linguistics because there are other approaches to the field. In particular, I agree with \citet{kodner2023linguistics}'s title, that ``linguistics will thrive in the 21st century''. There are now an incredible set of ideas to be integrated into a new, interdisciplinary science of language. It is especially exciting to see language at the center of AI, which it well deserves. But language wasn't put at the center of the world by the theories of generative syntax. The kind of linguistics that will thrive is one that is interdisciplinary, embraces new ideas, rethinks old assumptions, and integrates what we discover about the power of learning. There is no future for a corner of linguistics that slams the door and keeps repeating to itself that it, and it alone, has language figured out. 

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]}
\end{document}
